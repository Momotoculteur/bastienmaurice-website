## Introduction

Je vais te pr√©senter comment r√©aliser des sauvegardes de tes workloads stateful afin de gagner en r√©silience, ce qui pourrait t'√™tre utile dans plusieurs cas : 

- Pr√©parer une mise √† jour d'application
- Pr√©parer une mise √† jour de cluster
- Backup en cas de probl√®mes

!!! info
    A toi de g√©rer de ton c√¥t√© la partie installation de Velero/VeleroCLI. Je ne discuterais ici que de son utilisation

## Concepts cl√©s

### Le Stockage : PV et PVC
- **PV (PersistentVolume)** : C'est le morceau de stockage physique (ou virtuel) r√©el. Imaginez-le comme un disque dur branch√© dans le datacenter (ici, une instance Google Filestore). Il a une existence propre, ind√©pendante des Pods.
- **PVC (PersistentVolumeClaim)** : C'est la "revendication" ou le "ticket de r√©servation". Le d√©veloppeur cr√©e un PVC pour demander du stockage (ex: "Je veux 10Go"). Kubernetes cherche alors un PV disponible qui correspond et les lie ensemble (Binding).

### L'interface : CSI (Container Storage Interface)
C'est le standard universel qui permet √† Kubernetes de parler √† n'importe quel fournisseur de stockage (AWS EBS, Azure Disk, Google Filestore, etc.) sans avoir √† changer le code de Kubernetes lui-m√™me.

### Les Instantan√©s (Snapshots)
C'est ici que la magie op√®re pour le backup.

- **VolumeSnapshot** : C'est la demande de l'utilisateur. "Je veux prendre une photo de mon disque √† l'instant T". C'est un objet Kubernetes standard.
- **VolumeSnapshotContent** : C'est l'objet qui repr√©sente la photo r√©elle sur le stockage physique (l'identifiant du snapshot chez Google Cloud). C'est l'√©quivalent du PV, mais pour les snapshots.
- **VolumeSnapshotClass** : C'est le profil de configuration. Il dit √† Kubernetes : "Quand on demande un snapshot, utilise tel driver CSI (ex: Google Filestore) avec tels param√®tres". C'est l'√©quivalent de StorageClass mais pour les snapshots.

### Le chef d'orchestre : Velero

Velero est un outil open-source (plus pour longtemps, coucou VMWare üôÉ) pour backup, restore et migration de ressources Kubernetes et de volumes persistants.

Velero ne "copie" pas les donn√©es lui-m√™me octet par octet (dans ce mode CSI). Velero agit comme un chef d'orchestre :

1. Il contacte l'API Kubernetes pour demander un VolumeSnapshot.
2. Le driver CSI ex√©cute le snapshot sur l'infrastructure Cloud.
3. Velero sauvegarde la configuration (YAML) du PVC, du PV et du Snapshot dans un Bucket (Object Storage).
4. Au moment du restore, il recr√©e les objets et demande au CSI de cr√©er un nouveau volume √† partir du snapshot.

## Un peu de pratique
On va essayer de se faire dans cet ordre ci l'ensemble des manips du TP : 

- Cr√©er un PV/PVC avec des donn√©es simples (fichiers texte)
- Installer Velero et configurer les CSI snapshots
- Trigger un backup ou schedule via Velero CLI
- Restore dans un autre namespace
- V√©rifier avec un pod-inspector (un pod simple qui monte le PVC et permet d'inspecter les fichiers via kubectl exec)

### Pr√©-requis

- Un cluster GKE avec l'add-on Filestore CSI activ√© (pour mon exemple)
- Velero install√© sur le cluster
- Le feature flag **--features=EnableCSI** activ√© sur Velero (via helm, CLI...)

!!! warning
    je vais te montrer ici avec du NFS et le driver filestore.csi.storage.gke.io, attention velero ne fonctionne qu'avec du single share. Certains classe built-in √† Google Kubernetes Engine te propose du multi share (cf. enterprise)


### √âtape 1 : Configuration des Classes

On commence par d√©finir notre *StorageClass* qui sera consom√© par les PVC de nos apps : 

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: filestore-sc
provisioner: filestore.csi.storage.gke.io
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  tier: standard # selon ton tiers choisi
  network: default # idem aussi
```

On ajoute ensuite un *VolumeSnapshotClass* qui permettre de donner les bonnes indications √† Velero pour target le bon driver que l'on utlise pr√©c√©dement :

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: filestore-snap-class
  labels:
    velero.io/csi-volumesnapshot-class: "true" # Important pour que Velero le d√©tecte
driver: filestore.csi.storage.gke.io
deletionPolicy: Retain
```

### √âtape 2 : Cr√©ation de la Source et des Donn√©es
Ici pour l'example je vais fake le contenu d'un PV, mais √† toi de bien setter le PV/PVC de ton app statefullset.

Simple namespace ici : 

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: backup-source
```

On cr√©e un fake PVC pour le tp :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-test
  namespace: backup-source
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: filestore-sc
  resources:
    requests:
      storage: 100Gi
```

Un simple pod ici que l'on monte avec le volume pr√©c√©dent, avec une simple donn√©e que l'on va persister le plus simplement au monde :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: data-creator
  namespace: backup-source
spec:
  containers:
  - name: alpine
    image: alpine:latest
    command: ["/bin/sh", "-c"]
    # On √©crit la date et un message secret dans le fichier, puis on sleep infini pour garder le pod en vie
    args: ["echo 'Backup r√©ussi le: ' $(date) > /data/preuve.txt && echo 'Ceci est une donn√©e critique' >> /data/preuve.txt && sleep 3600"]
    volumeMounts:
    - mountPath: /data
      name: data-vol
  volumes:
  - name: data-vol
    persistentVolumeClaim:
      claimName: pv-test
```

### √âtape 3 : Ex√©cution du Backup via Velero CLI
C'est le moment critique ici, on va demander √† Velero de sauvegarder tout le namespace backup-source. Gr√¢ce √† l'int√©gration CSI, il va d√©tecter le PVC et cr√©er un VolumeSnapshot.

```bash
# Lancer le backup
velero backup create backup-cours-01 \

# V√©rifier les d√©tails du backup pour voir si le snapshot a √©t√© pris
velero backup describe backup-cours-01 --details
```

Si tu utilises comme moi les scheduleBackup de velero, tu peux aussi trigger la cr√©ation d'un backup depuis celui-ci via s: 

```bash
velero backup create --from-schedule example-schedule-01
```

!!! note
    Dans la sortie de la commande describe, tu devrais avoir la section "CSIVolumeSnapshots" de bien rempli


### √âtape 4 : Restauration dans un NOUVEAU Namespace
On va pouvoir ici restaurer les donn√©es dans un namespace diff√©rent nomm√© restore-target pour montrer la flexibilit√© de Velero


```bash
velero restore create <RESTORE_NAME> 
    --from-backup backup-cours-01 \
    --namespace-mappings backup-source:restore-target
```

### √âtape 5 : V√©rification avec le Pod Inspector
Le Pod data-creator a √©t√© restaur√©, mais pour l'exo, on va cr√©er un nouveau pod "inspecteur" dans le nouveau namespace pour aller lire le volume restaur√©

```yaml
# inspector.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-inspector
  namespace: restore-target 
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh", "-c", "sleep 3600"]
    volumeMounts:
    - mountPath: /data-restored
      name: restored-vol
  volumes:
  - name: restored-vol
    persistentVolumeClaim:
      claimName: pv-test # Le nom du PVC reste le m√™me, mais il est dans le nouveau namespace
```

Appliquez et v√©rifiez :

```bash
# Appliquer l'inspecteur
kubectl apply -f inspector.yaml

# Attendre qu'il d√©marre (le temps que le volume soit provisionn√© depuis le snapshot)
kubectl get pods -n restore-target -w

# LA PREUVE FINALE
echo "Lecture du fichier restaur√© :"
kubectl exec -n restore-target pod-inspector -- cat /data-restored/preuve.txt
```

## R√©sultat attendu

Si tout a fonctionn√©, la derni√®re commande affichera le texte √©crit √† l'√©tape 2 ü•≥ :

*Backup r√©ussi le: [Date d'origine] Ceci est une donn√©e critique*